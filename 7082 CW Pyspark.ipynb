{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.types as types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edu_Dataframe = spark.read.csv(\"states_all_extended.csv\", mode=\"DROPMALFORMED\",inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edu_Dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Edu_Dataframe.columns))\n",
    "print(Edu_Dataframe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A_A_A is total number of students, G01_A_A is total Grade 1 Students, etc. Only want to keep Grade 4 and Grade 8. \n",
    "Rems is removed columns, Reqs are required to keep.\n",
    "\n",
    "PRIMARY_KEY is removed as it is information otherwise duplicated in the columns STATE and YEAR.\n",
    "\n",
    "ENROLL is removed as the figure includes student numbers for grades other than 4 and 8.\n",
    "\n",
    "The breakdown of revenue sources (Federal, State, and Local) is also not relevant to the total revenue of the school\n",
    "\n",
    "Since we do not have gender breakdowns of results, aside from the results for \"All Ethnicities\", it is unnecessary to retain the gender split of each race in the enrollment columns. As such, all pairs of \"X_M\" and \"X_F\" can be summed into one \"X_A\" column.\n",
    "\n",
    "This allows us to reduce 266 columns down to 64 in a \"Simplified\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Edu_Dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sim_Edu_df = Edu_Dataframe\n",
    "\n",
    "Reqs = ['STATE', 'YEAR', 'TOTAL_REVENUE', 'TOTAL_EXPENDITURE', 'INSTRUCTION_EXPENDITURE', 'SUPPORT_SERVICES_EXPENDITURE', \\\n",
    "        'OTHER_EXPENDITURE', 'CAPITAL_OUTLAY_EXPENDITURE', 'G04_A_A', 'G08_A_A', 'G04_AM_F', 'G04_AM_M', 'G04_AS_F', \\\n",
    "        'G04_AS_M', 'G04_BL_F', 'G04_BL_M', 'G04_HI_F', 'G04_HI_M', 'G04_HP_F', 'G04_HP_M', 'G04_TR_F', 'G04_TR_M', \\\n",
    "        'G04_WH_F', 'G04_WH_M', 'G08_AM_F', 'G08_AM_M', 'G08_AS_F', 'G08_AS_M', 'G08_BL_F', 'G08_BL_M', 'G08_HI_F', \\\n",
    "        'G08_HI_M', 'G08_HP_F', 'G08_HP_M', 'G08_TR_F', 'G08_TR_M', 'G08_WH_F', 'G08_WH_M', 'G04_A_A_READING', \\\n",
    "        'G04_A_A_MATHEMATICS', 'G04_A_M_READING', 'G04_A_M_MATHEMATICS', 'G04_A_F_READING', 'G04_A_F_MATHEMATICS', \\\n",
    "        'G04_WH_A_READING', 'G04_WH_A_MATHEMATICS', 'G04_BL_A_READING', 'G04_BL_A_MATHEMATICS', 'G04_HI_A_READING', \\\n",
    "        'G04_HI_A_MATHEMATICS', 'G04_AS_A_READING', 'G04_AS_A_MATHEMATICS', 'G04_AM_A_READING', 'G04_AM_A_MATHEMATICS', \\\n",
    "        'G04_HP_A_READING', 'G04_HP_A_MATHEMATICS', 'G04_TR_A_READING', 'G04_TR_A_MATHEMATICS', 'G08_A_A_READING', \\\n",
    "        'G08_A_A_MATHEMATICS', 'G08_A_M_READING', 'G08_A_M_MATHEMATICS', 'G08_A_F_READING', 'G08_A_F_MATHEMATICS', \\\n",
    "        'G08_WH_A_READING', 'G08_WH_A_MATHEMATICS', 'G08_BL_A_READING', 'G08_BL_A_MATHEMATICS', 'G08_HI_A_READING', \\\n",
    "        'G08_HI_A_MATHEMATICS', 'G08_AS_A_READING', 'G08_AS_A_MATHEMATICS', 'G08_AM_A_READING', 'G08_AM_A_MATHEMATICS', \\\n",
    "        'G08_HP_A_READING', 'G08_HP_A_MATHEMATICS', 'G08_TR_A_READING', 'G08_TR_A_MATHEMATICS']\n",
    "\n",
    "\n",
    "Rems = ['PRIMARY_KEY', 'ENROLL', 'FEDERAL_REVENUE', 'STATE_REVENUE', 'LOCAL_REVENUE', 'A_A_A', 'G01_A_A', 'G02_A_A', \\\n",
    "        'G03_A_A', 'G05_A_A', 'G06_A_A', 'G07_A_A', 'G09_A_A', 'G10_A_A', 'G11_A_A', 'G12_A_A', 'KG_A_A', 'PK_A_A', \\\n",
    "        'G01-G08_A_A', 'G09-G12_A_A', 'G01_AM_F', 'G01_AM_M', 'G01_AS_F', 'G01_AS_M', 'G01_BL_F', 'G01_BL_M', 'G01_HI_F', \\\n",
    "        'G01_HI_M', 'G01_HP_F', 'G01_HP_M', 'G01_TR_F', 'G01_TR_M', 'G01_WH_F', 'G01_WH_M', 'G02_AM_F', 'G02_AM_M', \\\n",
    "        'G02_AS_F', 'G02_AS_M', 'G02_BL_F', 'G02_BL_M', 'G02_HI_F', 'G02_HI_M', 'G02_HP_F', 'G02_HP_M', 'G02_TR_F', \\\n",
    "        'G02_TR_M', 'G02_WH_F', 'G02_WH_M', 'G03_AM_F', 'G03_AM_M', 'G03_AS_F', 'G03_AS_M', 'G03_BL_F', 'G03_BL_M', \\\n",
    "        'G03_HI_F', 'G03_HI_M', 'G03_HP_F', 'G03_HP_M', 'G03_TR_F', 'G03_TR_M', 'G03_WH_F', 'G03_WH_M', 'G05_AM_F', \\\n",
    "        'G05_AM_M', 'G05_AS_F', 'G05_AS_M', 'G05_BL_F', 'G05_BL_M', 'G05_HI_F', 'G05_HI_M', 'G05_HP_F', 'G05_HP_M', \\\n",
    "        'G05_TR_F', 'G05_TR_M', 'G05_WH_F', 'G05_WH_M', 'G06_AM_F', 'G06_AM_M', 'G06_AS_F', 'G06_AS_M', 'G06_BL_F', \\\n",
    "        'G06_BL_M', 'G06_HI_F', 'G06_HI_M', 'G06_HP_F', 'G06_HP_M', 'G06_TR_F', 'G06_TR_M', 'G06_WH_F', 'G06_WH_M', \\\n",
    "        'G07_AM_F', 'G07_AM_M', 'G07_AS_F', 'G07_AS_M', 'G07_BL_F', 'G07_BL_M', 'G07_HI_F', 'G07_HI_M', 'G07_HP_F', \\\n",
    "        'G07_HP_M', 'G07_TR_F', 'G07_TR_M', 'G07_WH_F', 'G07_WH_M', 'G09_AM_F', 'G09_AM_M', 'G09_AS_F', 'G09_AS_M', \\\n",
    "        'G09_BL_F', 'G09_BL_M', 'G09_HI_F', 'G09_HI_M', 'G09_HP_F', 'G09_HP_M', 'G09_TR_F', 'G09_TR_M', 'G09_WH_F', \\\n",
    "        'G09_WH_M', 'G10_AM_F', 'G10_AM_M', 'G10_AS_F', 'G10_AS_M', 'G10_BL_F', 'G10_BL_M', 'G10_HI_F', 'G10_HI_M', \\\n",
    "        'G10_HP_F', 'G10_HP_M', 'G10_TR_F', 'G10_TR_M', 'G10_WH_F', 'G10_WH_M', 'G11_AM_F', 'G11_AM_M', 'G11_AS_F', \\\n",
    "        'G11_AS_M', 'G11_BL_F', 'G11_BL_M', 'G11_HI_F', 'G11_HI_M', 'G11_HP_F', 'G11_HP_M', 'G11_TR_F', 'G11_TR_M', \\\n",
    "        'G11_WH_F', 'G11_WH_M', 'G12_AM_F', 'G12_AM_M', 'G12_AS_F', 'G12_AS_M', 'G12_BL_F', 'G12_BL_M', 'G12_HI_F', \\\n",
    "        'G12_HI_M', 'G12_HP_F', 'G12_HP_M', 'G12_TR_F', 'G12_TR_M', 'G12_WH_F', 'G12_WH_M', 'KG_AM_F', 'KG_AM_M', \\\n",
    "        'KG_AS_F', 'KG_AS_M', 'KG_BL_F', 'KG_BL_M', 'KG_HI_F', 'KG_HI_M', 'KG_HP_F', 'KG_HP_M', 'KG_TR_F', 'KG_TR_M', \\\n",
    "        'KG_WH_F', 'KG_WH_M', 'PK_AM_F', 'PK_AM_M', 'PK_AS_F', 'PK_AS_M', 'PK_BL_F', 'PK_BL_M', 'PK_HI_F', 'PK_HI_M', \\\n",
    "        'PK_HP_F', 'PK_HP_M', 'PK_TR_F', 'PK_TR_M', 'PK_WH_F', 'PK_WH_M']\n",
    "\n",
    "for e in Rems:\n",
    "    Sim_Edu_df = Sim_Edu_df.drop(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Edu_Dataframe.columns))\n",
    "print(len(Sim_Edu_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eth = [\"AM_\", \"AS_\", \"BL_\", \"HI_\", \"HP_\", \"TR_\", \"WH_\"] \n",
    "\n",
    "print(len(Sim_Edu_df.columns))\n",
    "\n",
    "for e in Eth:\n",
    "    Sim_Edu_df = Sim_Edu_df.withColumn(\"G04_\"+ e +\"A\", func.col(\"G04_\" + e + \"M\")+func.col(\"G04_\" + e + \"F\"))\n",
    "    Sim_Edu_df = Sim_Edu_df.withColumn(\"G08_\"+ e +\"A\", func.col(\"G08_\" + e + \"M\")+func.col(\"G08_\" + e + \"F\"))\n",
    "    Sim_Edu_df = Sim_Edu_df.drop(\"G04_\"+ e + \"M\")\n",
    "    Sim_Edu_df = Sim_Edu_df.drop(\"G04_\"+ e + \"F\")\n",
    "    Sim_Edu_df = Sim_Edu_df.drop(\"G08_\"+ e + \"M\")\n",
    "    Sim_Edu_df = Sim_Edu_df.drop(\"G08_\"+ e + \"F\")\n",
    "\n",
    "print(len(Sim_Edu_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Sim_Edu_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, from the below cell we can see that there is no data entered for 2018, even though 2019 seems to be included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Years = Sim_Edu_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"]\n",
    "print(\"These are the unique years: \" + str(Unique_Years))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "Unique_States = Sim_Edu_df.select(func.sort_array(*[func.collect_set(\"STATE\")]).alias(\"STATE\")).collect()[0][\"STATE\"]\n",
    "print(\"These are the unique States: \" + str(Unique_States))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Unique_Years))\n",
    "\n",
    "print(len(Unique_States))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that there are 53 US states listed. It appears that DODEA (Department of Defence Education Activity), National, and The District Of Columbia have all be included on the list. The District of Columbia is not officially a state due to the US constitution, however for our purposes, it functions identically, and so shall be included.\n",
    "The number of occurences of each state in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in Unique_States:\n",
    "    print(str(c) + \": \" + str(Sim_Edu_df.select(\"STATE\").where((Sim_Edu_df[\"STATE\"] == c)).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 33 instances (The number of years the dataset covers) of each \"State\", besides National and DODEA, which is fortunate as we plan to remove those, using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Clean_Edu = Sim_Edu_df\n",
    "for c in Unique_States:\n",
    "    if Sim_Edu_df.select(\"STATE\").where((Sim_Edu_df[\"STATE\"] == c)).count() < 33:\n",
    "        State_Clean_Edu = State_Clean_Edu.where(Sim_Edu_df[\"STATE\"] != c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_Clean_Edu.select(\"STATE\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unique_Years_2 = State_Clean_Edu.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"]\n",
    "print(\"These are the unique years: \" + str(Unique_Years_2))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "Unique_States_2 = State_Clean_Edu.select(func.sort_array(*[func.collect_set(\"STATE\")]).alias(\"STATE\")).collect()[0][\"STATE\"]\n",
    "print(\"These are the unique States: \" + str(Unique_States_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Unique_Years_2))\n",
    "\n",
    "print(len(Unique_States_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of null values per column are listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_Counts = {}\n",
    "\n",
    "for c in State_Clean_Edu.columns:\n",
    "    Null_Counts[c] = State_Clean_Edu.filter( \\\n",
    "    (State_Clean_Edu[c] == \"\") | State_Clean_Edu[c].isNull() | func.isnan(State_Clean_Edu[c]) \\\n",
    "    ).count()\n",
    "\n",
    "print(Null_Counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "State_Clean_Edu.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2019 year data does not contain the Revenue, Expenditure, or Student Counts for each state, only showing the average score for each demographic. As such, there is not enough information to compare this against, and the rows will be purged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_G04_df = State_Clean_Edu.filter(State_Clean_Edu[\"G04_A_A\"].isNull() == True)\n",
    "Null_G08_df = State_Clean_Edu.filter(State_Clean_Edu[\"G08_A_A\"].isNull() == True)\n",
    "Null_Rev_df = State_Clean_Edu.filter(State_Clean_Edu[\"TOTAL_REVENUE\"].isNull() == True)\n",
    "Null_Exp_df = State_Clean_Edu.filter(State_Clean_Edu[\"TOTAL_EXPENDITURE\"].isNull() == True)\n",
    "\n",
    "#Unique years in Null_G04_df\n",
    "print(Null_G04_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"])\n",
    "\n",
    "#Unique years in Null_G08_df\n",
    "print(Null_G08_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"])\n",
    "\n",
    "#Unique years in Null_Rev_df\n",
    "print(Null_Rev_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"])\n",
    "\n",
    "#Unique years in Null_Exp_df\n",
    "print(Null_Exp_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"])\n",
    "\n",
    "Clean_df = State_Clean_Edu.filter(State_Clean_Edu[\"G04_A_A\"].isNull() == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_df = Clean_df.where(Clean_df[\"YEAR\"] != 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Null = 0\n",
    "\n",
    "for c in Null_G04_df.columns:\n",
    "    Num_Null += Clean_df.where(Clean_df[c].isNull()).count()\n",
    "\n",
    "print(Num_Null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, there are 357 null values in the financial columns. This is equal to 51 * 7, so we would expect all states to be missing fiscal data for 7 years. This is found to be correct in the following cells, and the incomplete years removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Clean_df.where(Clean_df[\"TOTAL_EXPENDITURE\"].isNull()).count())\n",
    "print(Clean_df.where(Clean_df[\"TOTAL_REVENUE\"].isNull()).count())\n",
    "print(Clean_df.where(Clean_df[\"INSTRUCTION_EXPENDITURE\"].isNull()).count())\n",
    "print(Clean_df.where(Clean_df[\"SUPPORT_SERVICES_EXPENDITURE\"].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in [\"TOTAL_REVENUE\", \"TOTAL_EXPENDITURE\", \"INSTRUCTION_EXPENDITURE\", \"SUPPORT_SERVICES_EXPENDITURE\"]:\n",
    "    Null_REV_df = Clean_df.filter(Clean_df[c].isNull() == True)\n",
    "    print(Null_REV_df.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_df = Clean_df.filter(~Clean_df[\"YEAR\"].isin([1986, 1987, 1988, 1989, 1990, 1991, 2017]))\n",
    "\n",
    "Clean_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanPandas = Clean_df.toPandas()\n",
    "\n",
    "CleanPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_Counts = {}\n",
    "\n",
    "for c in Clean_df.columns:\n",
    "    Null_Counts[c] = Clean_df.filter((Clean_df[c] == \"\") | Clean_df[c].isNull() | func.isnan(Clean_df[c])).count()\n",
    "\n",
    "print(Null_Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num of Cols of Results: \" + str(len(['G04_A_A_READING', 'G04_A_A_MATHEMATICS', 'G04_A_M_READING', 'G04_A_M_MATHEMATICS', 'G04_A_F_READING', 'G04_A_F_MATHEMATICS', 'G04_WH_A_READING', 'G04_WH_A_MATHEMATICS', 'G04_BL_A_READING', 'G04_BL_A_MATHEMATICS', 'G04_HI_A_READING', 'G04_HI_A_MATHEMATICS', 'G04_AS_A_READING', 'G04_AS_A_MATHEMATICS', 'G04_AM_A_READING', 'G04_AM_A_MATHEMATICS', 'G04_HP_A_READING', 'G04_HP_A_MATHEMATICS', 'G04_TR_A_READING', 'G04_TR_A_MATHEMATICS', 'G08_A_A_READING', 'G08_A_A_MATHEMATICS', 'G08_A_M_READING', 'G08_A_M_MATHEMATICS', 'G08_A_F_READING', 'G08_A_F_MATHEMATICS', 'G08_WH_A_READING', 'G08_WH_A_MATHEMATICS', 'G08_BL_A_READING', 'G08_BL_A_MATHEMATICS', 'G08_HI_A_READING', 'G08_HI_A_MATHEMATICS', 'G08_AS_A_READING', 'G08_AS_A_MATHEMATICS', 'G08_AM_A_READING', 'G08_AM_A_MATHEMATICS', 'G08_HP_A_READING', 'G08_HP_A_MATHEMATICS', 'G08_TR_A_READING', 'G08_TR_A_MATHEMATICS'])))\n",
    "\n",
    "print(\"Num of Cols of Student Nums: \" + str(len(['G04_A_A', 'G08_A_A', 'G04_AM_A', 'G08_AM_A', 'G04_AS_A', 'G08_AS_A', 'G04_BL_A', 'G08_BL_A', 'G04_HI_A', 'G08_HI_A', 'G04_HP_A', 'G08_HP_A', 'G04_TR_A', 'G08_TR_A', 'G04_WH_A', 'G08_WH_A'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stud_Num_df = Clean_df['G04_A_A', 'G08_A_A', 'G04_AM_A', 'G08_AM_A', 'G04_AS_A', 'G08_AS_A', 'G04_BL_A', 'G08_BL_A', 'G04_HI_A', 'G08_HI_A', 'G04_HP_A', 'G08_HP_A', 'G04_TR_A', 'G08_TR_A', 'G04_WH_A', 'G08_WH_A']\n",
    "Result_df = Clean_df['G04_A_A_READING', 'G04_A_A_MATHEMATICS', 'G04_A_M_READING', 'G04_A_M_MATHEMATICS', 'G04_A_F_READING', 'G04_A_F_MATHEMATICS', 'G04_WH_A_READING', 'G04_WH_A_MATHEMATICS', 'G04_BL_A_READING', 'G04_BL_A_MATHEMATICS', 'G04_HI_A_READING', 'G04_HI_A_MATHEMATICS', 'G04_AS_A_READING', 'G04_AS_A_MATHEMATICS', 'G04_AM_A_READING', 'G04_AM_A_MATHEMATICS', 'G04_HP_A_READING', 'G04_HP_A_MATHEMATICS', 'G04_TR_A_READING', 'G04_TR_A_MATHEMATICS', 'G08_A_A_READING', 'G08_A_A_MATHEMATICS', 'G08_A_M_READING', 'G08_A_M_MATHEMATICS', 'G08_A_F_READING', 'G08_A_F_MATHEMATICS', 'G08_WH_A_READING', 'G08_WH_A_MATHEMATICS', 'G08_BL_A_READING', 'G08_BL_A_MATHEMATICS', 'G08_HI_A_READING', 'G08_HI_A_MATHEMATICS', 'G08_AS_A_READING', 'G08_AS_A_MATHEMATICS', 'G08_AM_A_READING', 'G08_AM_A_MATHEMATICS', 'G08_HP_A_READING', 'G08_HP_A_MATHEMATICS', 'G08_TR_A_READING', 'G08_TR_A_MATHEMATICS']\n",
    "\n",
    "OnlyNull_Results = Result_df.withColumn('ResultNulls', sum(Result_df[col].isNull().cast('int') for col in Result_df.columns))\n",
    "\n",
    "OnlyNull_StudentNo = Stud_Num_df.withColumn('StudNoNulls', sum(Stud_Num_df[col].isNull().cast('int') for col in Stud_Num_df.columns))\n",
    "\n",
    "Total_Nulls = Clean_df.withColumn('ResultNulls', sum(Result_df[col].isNull().cast('int') for col in Result_df.columns))\n",
    "\n",
    "Total_Nulls = Total_Nulls.withColumn('StudNoNulls', sum(Stud_Num_df[col].isNull().cast('int') for col in Stud_Num_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NullResultPD = OnlyNull_Results.toPandas()\n",
    "NullStudentNoPD = OnlyNull_StudentNo.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When G04 and G08's Mathematics and Reading results are missing, all demographic result data is also absent, so we cannot calculate it from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Test = OnlyNull_Results.where(OnlyNull_Results[\"G04_A_A_MATHEMATICS\"].isNull() & OnlyNull_Results[\"G04_A_A_READING\"].isNull() & OnlyNull_Results[\"G08_A_A_MATHEMATICS\"].isNull() & OnlyNull_Results[\"G08_A_A_READING\"].isNull())\n",
    "\n",
    "NullResultPDTest = Test.toPandas()\n",
    "\n",
    "Null_Counter = 0\n",
    "\n",
    "for c in Test.columns:\n",
    "    Null_Counter += Test.filter((Test[c] == \"\") | Test[c].isNull() | func.isnan(Test[c])).count()\n",
    "\n",
    "print(Null_Counter)\n",
    "    \n",
    "NullResultPDTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "711*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test2 = OnlyNull_Results.where(~(OnlyNull_Results[\"G04_A_A_MATHEMATICS\"].isNull() & OnlyNull_Results[\"G04_A_A_READING\"].isNull() & OnlyNull_Results[\"G08_A_A_MATHEMATICS\"].isNull() & OnlyNull_Results[\"G08_A_A_READING\"].isNull()))\n",
    "\n",
    "SomeResultPDTest = Test2.toPandas()\n",
    "\n",
    "Null_Counter = 0\n",
    "\n",
    "for c in Test.columns:\n",
    "    Null_Counter += Test2.filter((Test2[c] == \"\") | Test2[c].isNull() | func.isnan(Test2[c])).count()\n",
    "\n",
    "print(Null_Counter)\n",
    "    \n",
    "SomeResultPDTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalNullPD = Total_Nulls.toPandas()\n",
    "print(Total_Nulls.columns)\n",
    "TotalNullPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteredPD = Total_Nulls.where((Total_Nulls[\"ResultNulls\"] == 40) & (Total_Nulls[\"StudNoNulls\"] == 14)).toPandas()\n",
    "\n",
    "FilteredPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging results in column\n",
    "\n",
    "def Means_Of_Cols(df, Incomplete_cols, verbose=False):\n",
    "    Col_Means=[]\n",
    "    for c in Incomplete_cols:\n",
    "        Mean = df.select(func.avg(df[c]))\n",
    "        Avg = Mean.columns[0]\n",
    "        Result = Mean.rdd.map(lambda row : row[Avg]).collect()\n",
    "        \n",
    "        if (verbose==True): print(Mean.columns[0], result[0])\n",
    "        Col_Means.append([c, Result[0]])    \n",
    "    return Col_Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_Res_Data = [\"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\"]\n",
    "\n",
    "print(\"Mean values: \",  Means_Of_Cols(Clean_df,Missing_Res_Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Null with mean\n",
    "\n",
    "def Null_to_mean(df, numeric_cols):\n",
    "    col_mean = Means_Of_Cols(df, numeric_cols) \n",
    "    \n",
    "    for col, mean in col_mean:\n",
    "        df = df.withColumn(col, func.when(df[col].isNull(), func.lit(mean)).otherwise(df[col]))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_By_State = Clean_df.where(Clean_df[\"STATE\"] == \"NotAState\")\n",
    "\n",
    "for s in Unique_States_2:\n",
    "    temp_df = Null_to_mean(Clean_df.where(Clean_df[\"STATE\"] == s), Missing_Res_Data)\n",
    "    Clean_By_State = Clean_By_State.union(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_By_State.select(\"G04_A_A_MATHEMATICS\").where(Clean_By_State[\"G04_A_A_MATHEMATICS\"].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_By_State.select(\"STATE\", \"YEAR\", \"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Null_Counts = {}\n",
    "\n",
    "for c in Clean_By_State.columns:\n",
    "    Null_Counts[c] = Clean_By_State.filter((Clean_By_State[c] == \"\") | Clean_By_State[c].isNull() | func.isnan(Clean_By_State[c])).count()\n",
    "\n",
    "print(Null_Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Years_in_Clean = Clean_By_State.select(func.sort_array(*[func.collect_set(\"YEAR\")]).alias(\"YEAR\")).collect()[0][\"YEAR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in Years_in_Clean:\n",
    "    print(str(y) + \": \" + str(Clean_By_State.where((func.col(\"YEAR\") == y) & (func.col(\"G04_A_A_MATHEMATICS\").isNull() == False)).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ByYear = Clean_By_State.groupBy('YEAR').agg(func.sum('TOTAL_REVENUE').alias('REVENUE_BY_YEAR'), \\\n",
    "(func.sum('G04_A_A') + func.sum('G08_A_A')).alias('STUDENT_BY_YEAR'))\n",
    "     \n",
    "ByYear = ByYear.withColumn(\"Dollars_Per_Students\", (func.col(\"REVENUE_BY_YEAR\") / func.col(\"STUDENT_BY_YEAR\")))\n",
    "    \n",
    "Temp = Clean_By_State.groupBy('YEAR').agg((func.sum('G04_A_A_READING')).alias('Average_G04_Reading'), \\\n",
    "(func.sum('G08_A_A_READING')).alias('Average_G08_Reading'), \\\n",
    "(func.sum('G04_A_A_MATHEMATICS')).alias('Average_G04_Mathematics'), \\\n",
    "(func.sum('G08_A_A_MATHEMATICS')).alias('Average_G08_Mathematics'), \\\n",
    ")  \n",
    "\n",
    "Num_G04_Maths_Datapoints = []\n",
    "Num_G04_Read_Datapoints = []\n",
    "Num_G08_Maths_Datapoints = []\n",
    "Num_G08_Read_Datapoints = []\n",
    "\n",
    "for y in Years_in_Clean:\n",
    "    Num_G04_Maths_Datapoints.append(51)\n",
    "    Num_G04_Read_Datapoints.append(51)\n",
    "    Num_G08_Maths_Datapoints.append(51)\n",
    "    Num_G08_Read_Datapoints.append(51)\n",
    "      \n",
    "G04_Maths_Temps = sqlContext.createDataFrame(zip(Years_in_Clean, Num_G04_Maths_Datapoints), \\\n",
    "         schema = [\"Year\", \"G04_Maths_Datapoints\"])\n",
    "G04_Read_Temps = sqlContext.createDataFrame(zip(Years_in_Clean, Num_G04_Read_Datapoints), \\\n",
    "         schema = [\"Year\", \"G04_Read_Datapoints\"])\n",
    "G08_Maths_Temps = sqlContext.createDataFrame(zip(Years_in_Clean, Num_G08_Maths_Datapoints), \\\n",
    "         schema = [\"Year\", \"G08_Maths_Datapoints\"])\n",
    "G08_Read_Temps = sqlContext.createDataFrame(zip(Years_in_Clean, Num_G08_Read_Datapoints), \\\n",
    "         schema = [\"Year\", \"G08_Read_Datapoints\"])\n",
    "\n",
    "\n",
    "ByYear = ByYear.join(Temp, on= [\"YEAR\"], how= \"inner\")\n",
    "\n",
    "ByYear = ByYear.join(G04_Maths_Temps, on=[\"YEAR\"], how=\"inner\")\n",
    "ByYear = ByYear.join(G04_Read_Temps, on=[\"YEAR\"], how=\"inner\")\n",
    "ByYear = ByYear.join(G08_Maths_Temps, on=[\"YEAR\"], how=\"inner\")\n",
    "ByYear = ByYear.join(G08_Read_Temps, on=[\"YEAR\"], how=\"inner\")\n",
    "\n",
    "\n",
    "ByYear = ByYear.withColumn(\"Average_G04_Mathematics\", func.col(\"Average_G04_Mathematics\") / func.col(\"G04_Maths_Datapoints\"))\n",
    "ByYear = ByYear.withColumn(\"Average_G04_Reading\", func.col(\"Average_G04_Reading\") / func.col(\"G04_Read_Datapoints\"))\n",
    "ByYear = ByYear.withColumn(\"Average_G08_Mathematics\", func.col(\"Average_G08_Mathematics\") / func.col(\"G08_Maths_Datapoints\"))\n",
    "ByYear = ByYear.withColumn(\"Average_G08_Reading\", func.col(\"Average_G08_Reading\") / func.col(\"G08_Read_Datapoints\"))\n",
    "\n",
    "\n",
    "ByYear = ByYear.drop(\"G04_Maths_Datapoints\")\n",
    "ByYear = ByYear.drop(\"G04_Read_Datapoints\")\n",
    "ByYear = ByYear.drop(\"G08_Maths_Datapoints\")\n",
    "ByYear = ByYear.drop(\"G08_Read_Datapoints\")\n",
    "\n",
    "ByYear = ByYear.orderBy(\"YEAR\")\n",
    "PdByYear = ByYear.toPandas()\n",
    "\n",
    "PdByYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_List = Sim_Edu_df.select(func.sort_array(*[func.collect_set(\"STATE\")]).alias(\"STATE\")).collect()[0][\"STATE\"]\n",
    "\n",
    "ByState = Clean_By_State.groupBy('STATE').agg(func.sum('TOTAL_REVENUE').alias('REVENUE_BY_STATE'), \\\n",
    "(func.sum('G04_A_A') + func.sum('G08_A_A')).alias('STUDENT_BY_STATE'))\n",
    "     \n",
    "ByState = ByState.withColumn(\"Dollars_Per_Students\", (func.col(\"REVENUE_BY_STATE\") / func.col(\"STUDENT_BY_STATE\")))\n",
    "    \n",
    "Temp = Clean_By_State.groupBy('STATE').agg((func.sum('G04_A_A_READING')).alias('Average_G04_Reading'), \\\n",
    "(func.sum('G08_A_A_READING')).alias('Average_G08_Reading'), \\\n",
    "(func.sum('G04_A_A_MATHEMATICS')).alias('Average_G04_Mathematics'), \\\n",
    "(func.sum('G08_A_A_MATHEMATICS')).alias('Average_G08_Mathematics'), \\\n",
    ")  \n",
    "\n",
    "Num_G04_Maths_Datapoints = []\n",
    "Num_G04_Read_Datapoints = []\n",
    "Num_G08_Maths_Datapoints = []\n",
    "Num_G08_Read_Datapoints = []\n",
    "\n",
    "for s in State_List:\n",
    "    Num_G04_Maths_Datapoints.append(25)\n",
    "    Num_G04_Read_Datapoints.append(25)\n",
    "    Num_G08_Maths_Datapoints.append(25)\n",
    "    Num_G08_Read_Datapoints.append(25)\n",
    "        \n",
    "G04_Maths_Temps = sqlContext.createDataFrame(zip(State_List, Num_G04_Maths_Datapoints), \\\n",
    "        schema = [\"State\", \"G04_Maths_Datapoints\"])\n",
    "G04_Read_Temps = sqlContext.createDataFrame(zip(State_List, Num_G04_Read_Datapoints), \\\n",
    "        schema = [\"State\", \"G04_Read_Datapoints\"])\n",
    "G08_Maths_Temps = sqlContext.createDataFrame(zip(State_List, Num_G08_Maths_Datapoints), \\\n",
    "        schema = [\"State\", \"G08_Maths_Datapoints\"])\n",
    "G08_Read_Temps = sqlContext.createDataFrame(zip(State_List, Num_G08_Read_Datapoints), \\\n",
    "        schema = [\"State\", \"G08_Read_Datapoints\"])\n",
    "\n",
    "\n",
    "ByState = ByState.join(Temp, on= [\"STATE\"], how= \"inner\")\n",
    "\n",
    "ByState = ByState.join(G04_Maths_Temps, on=[\"STATE\"], how=\"inner\")\n",
    "ByState = ByState.join(G04_Read_Temps, on=[\"STATE\"], how=\"inner\")\n",
    "ByState = ByState.join(G08_Maths_Temps, on=[\"STATE\"], how=\"inner\")\n",
    "ByState = ByState.join(G08_Read_Temps, on=[\"STATE\"], how=\"inner\")\n",
    "\n",
    "\n",
    "ByState = ByState.withColumn(\"Average_G04_Mathematics\", func.col(\"Average_G04_Mathematics\") / func.col(\"G04_Maths_Datapoints\"))\n",
    "ByState = ByState.withColumn(\"Average_G04_Reading\", func.col(\"Average_G04_Reading\") / func.col(\"G04_Read_Datapoints\"))\n",
    "ByState = ByState.withColumn(\"Average_G08_Mathematics\", func.col(\"Average_G08_Mathematics\") / func.col(\"G08_Maths_Datapoints\"))\n",
    "ByState = ByState.withColumn(\"Average_G08_Reading\", func.col(\"Average_G08_Reading\") / func.col(\"G08_Read_Datapoints\"))\n",
    "\n",
    "\n",
    "ByState = ByState.drop(\"G04_Maths_Datapoints\")\n",
    "ByState = ByState.drop(\"G04_Read_Datapoints\")\n",
    "ByState = ByState.drop(\"G08_Maths_Datapoints\")\n",
    "ByState = ByState.drop(\"G08_Read_Datapoints\")\n",
    "\n",
    "ByState = ByState.orderBy(\"STATE\")\n",
    "PdByState = ByState.toPandas()\n",
    "\n",
    "PdByState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_By_State.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"Cleaned_Data\")\n",
    "\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "\n",
    "list_status = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(\"Cleaned_Data/\"))\n",
    "\n",
    "filePath = \"Cleaned_Data/\"\n",
    "for file in list_status: \n",
    "    if file.getPath().getName().startswith('part-') == True:\n",
    "        fileName = file.getPath().getName()\n",
    "\n",
    "fs.rename(Path(filePath+fileName), Path(filePath+\"Cleaned_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ByYear.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"Yearly_Data\")\n",
    "\n",
    "list_status2 = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(\"Yearly_Data/\"))\n",
    "\n",
    "filePath2 = \"Yearly_Data/\"\n",
    "for file in list_status2: \n",
    "    if file.getPath().getName().startswith('part-') == True:\n",
    "        fileName2 = file.getPath().getName()\n",
    "        \n",
    "fs.rename(Path(filePath2+fileName2), Path(filePath2+\"Yearly_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ByState.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"Stately_Data\")\n",
    "\n",
    "list_status3 = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(\"Stately_Data/\"))\n",
    "\n",
    "filePath3 = \"Stately_Data/\"\n",
    "for file in list_status3: \n",
    "    if file.getPath().getName().startswith('part-') == True:\n",
    "        fileName3 = file.getPath().getName()\n",
    "        \n",
    "fs.rename(Path(filePath3+fileName3), Path(filePath3+\"Stately_Data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "Clean_Corr = Clean_By_State.toPandas().corr()\n",
    "\n",
    "Clean_Corr2 = Clean_Corr.loc[Clean_Corr[\"TOTAL_REVENUE\"] > 0.75]\n",
    "\n",
    "Clean_Corr3 = Clean_Corr.loc[(Clean_Corr[\"G04_A_A_READING\"] > 0.75) | (Clean_Corr[\"G04_A_A_MATHEMATICS\"] > 0.75) | \\\n",
    "            (Clean_Corr[\"G08_A_A_READING\"] > 0.75) | (Clean_Corr[\"G08_A_A_MATHEMATICS\"] > 0.75)] \\\n",
    "            [[\"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\"]]\n",
    "\n",
    "Clean_Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Corr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Scatter = Clean_df.select(\"YEAR\", \"TOTAL_REVENUE\", \"G04_A_A\", \"G08_A_A\", \"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "Clean_df_plot = sns.pairplot(Clean_Scatter.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "Rev_vs_Maths = Clean_By_State.select(Clean_By_State[\"Total_Revenue\"],Clean_By_State[\"G04_A_A_MATHEMATICS\"].alias('G04_Maths'))\n",
    "train, test = Rev_vs_Maths.randomSplit([0.7,0.3])\n",
    "\n",
    "#Vectorise Total_Revenue, rename as Revenue\n",
    "assembler = VectorAssembler().setInputCols([\"Total_Revenue\",]).setOutputCol(\"Revenue\")\n",
    "train = assembler.transform(train)\n",
    "\n",
    "# Keep Result and Vectorised Revenue column, drop original Total_Revenue\n",
    "train = train.select(\"Revenue\",\"G04_Maths\")\n",
    "train.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'Revenue', labelCol='G04_Maths', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model = lr.fit(train)\n",
    "test = assembler.transform(test)\n",
    "test = test.select('Revenue', 'G04_Maths')\n",
    "test = model.transform(test)\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol = \"G04_Maths\")\n",
    "\n",
    "#R squared test\n",
    "print(\"R2 Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"r2\"})))\n",
    "\n",
    "#Mean Squared Error\n",
    "print(\"MSE Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"mse\"})))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "print(\"RMSE Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"rmse\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "train, test = Clean_By_State.randomSplit([0.7,0.3])\n",
    "assembler = VectorAssembler().setInputCols([\"G04_A_A\", \"G08_A_A\", \"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \\\n",
    "                                            \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\"]).setOutputCol('features')\n",
    "train = assembler.transform(train)\n",
    "train = train.select(\"features\",\"Total_Revenue\")\n",
    "train.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='Total_Revenue', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model = lr.fit(train)\n",
    "test = assembler.transform(test)\n",
    "test = test.select('features', 'Total_Revenue')\n",
    "test = model.transform(test)\n",
    "test.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol = \"Total_Revenue\")\n",
    "\n",
    "#R squared test\n",
    "print(\"R2 Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"r2\"})))\n",
    "\n",
    "#Mean Squared Error\n",
    "print(\"MSE Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"mse\"})))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "print(\"RMSE Test: \" + str(evaluator.evaluate(test,{evaluator.metricName: \"rmse\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([\"G04_A_A\", \"G08_A_A\", \"G04_A_A_READING\", \"G04_A_A_MATHEMATICS\", \"G08_A_A_READING\", \"G08_A_A_MATHEMATICS\"], \\\n",
    "         model.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
